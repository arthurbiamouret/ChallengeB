---
title: "Challenge B"
author: "Luisa Van der Ploeg & Arthur Biamouret"
date: "5 décembre 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

You can find all the data and the documents on github, just click [here](https://github.com/arthurbiamouret?tab=repositories).  



## Task 1B - Predicting house prices in Ames, Iowa (continued)

We first download all the packages needed for the Task 1B.
```{r installing the packages}
library(tidyverse)
library(randomForest)
```


We open the data that we are going to use in this task.
```{r data , echo = TRUE}

train <- read.csv(file.choose(), header = T, dec = ".")
test <- read.csv(file.choose(), header = T, dec = ".")

```


We take out all the missing data from our training data using the code of the correction of challenge A.
```{r missing data, echo= TRUE}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

```

Our training data is now cleaned.  

#### Step 1: *Choose a ML technique : non-parametric kernel estimation, random forests, etc. . . Give a brief intuition of how it works. (1 points)*  

**Random forest** is a method that uses multiple learning algorithms to obtain better predictive performance than could be obtained from one of the learning algorithms alone. It is a method that operate by constructing multiple decision trees and giving the class (classification or regression) of each tree. It is usefull to correct for decision trees' habit of overfitting to their training set. It does several independant models and calculte the mean of all of them in order to reduce the forecast error made when we do regression. This is why the more trees there are, the better the result.


#### Step 2: *Train the chosen technique on the training data. Hint : packages np for non-parametric regressions, randomForest for random forests. Don't use the variable Id as a feature. (2 points)*  

Here we train Random forest on our training data. Thanks to this technique we can look at the importance of each variables in those predictions.  
```{r random forest training, echo=TRUE}
M1 <- randomForest(train$SalePrice ~ .-Id, data = train)
print(M1)
varImpPlot(M1)
M1$importance
```

#### Step 3: *Make predictions on the test data, and compare them to the predictions of a linear regression of your choice. (2 points)*  

We first start by removing all the missing values in our testing data using exactly the same method as the one for the training data.  
```{r random forest prediction, echo=TRUE}
remove.vars <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars))

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

test <- test %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE, is.na(MSZoning) == FALSE, is.na(Utilities) == FALSE, is.na(BsmtQual) == FALSE, is.na(BsmtCond) == FALSE, is.na(KitchenQual) == FALSE, is.na(Functional) == FALSE, is.na(GarageYrBlt) == FALSE, is.na(GarageFinish) == FALSE, is.na(GarageCars) == FALSE, is.na(GarageArea) == FALSE,is.na(GarageQual) == FALSE, is.na(GarageCond) == FALSE, is.na(SaleType) == FALSE)

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
```

Because the function *random forest* is not confortable with the differences in levels, we need to adjust the levels of some variables between our training and our testing data. We adjust the testing data to the training one because the levels are always higher in the training one. We can do that using the function *levels*.  
```{r adjusting the levels}
levels(test$Utilities) <- levels(train$Utilities)
levels(test$Condition2) <- levels(train$Condition2)
levels(test$HouseStyle) <- levels(train$HouseStyle)
levels(test$RoofMatl) <- levels(train$RoofMatl)
levels(test$Exterior1st) <- levels(train$Exterior1st)
levels(test$Exterior2nd) <- levels(train$Exterior2nd)
levels(test$Electrical) <- levels(train$Electrical)
levels(test$GarageQual) <- levels(train$GarageQual)
levels(test$Heating) <- levels(train$Heating)
```  

Then we do our predictions using the function *predict*.  
```{r predictions}
prediction2 <- data.frame(Id = test$Id, SalePrice_predict = predict(M1, test, type="response"))
write.csv(x = prediction2, file = "predictions2.csv", na = "NA", quote = FALSE, row.names = FALSE)

summary(prediction2)
```

To compare our predictions to the ones of the linear regression, we do exactly the same model as the one made in the correction of challenge A. We use this model to do our predictions.   
```{r predictions of challenge A}
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)

prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)

summary(prediction)
```

To compare them together, we plot them on a same graph.  
```{r plot the predictions}
ggplot()+geom_point(mapping = aes(x=prediction2$Id, y=prediction2$SalePrice_predict, colour="blue"))+
  geom_point(mapping = aes(x=prediction$Id, y=prediction$SalePrice_predict, colour="red"))
```

Thanks to this graph, we can see that our random forest predictions (red) has a tendency to predict much more high values: at the top of the graph there are only red points and no blue ones. Reversly, the other prediction, lets call it the classic one (blue), has the tendency to predict lower values. We can observe at the bottom of the graph that there are only bue points.  



******


## Task 2 A - Overfitting in Machine Learning (continued)

We start by uploading the packages needed for this task.  
```{r upload the packages, echo=TRUE}
library(np)
library(caret)
library(tidyverse)
```

We will use the same data as in challenge A. This is why we create them in exactly the same way (by taking the same code as the one of the challenge A's correction).  
```{r overfit, echo = TRUE, include = TRUE}
set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test"))

training <- df %>% filter(which.data == "training")
training <- data.frame(training)
test <- df %>% filter(which.data == "test")
test <- data.frame(test)
```

#### Step 1: *Estimate a low-flexibility local linear model on the training data. For that, you can use function npreg the package np. Choose ll for the method (local linear), and a bandwidth of 0.5; Call this model ll.fit.lowflex*  
```{r step1B, echo=TRUE}
ll.fit.lowflex <- npreg(y ~ x, bws = 0.5, method="ll", data=training)
```

#### Step 2: *Estimate a high-flexibility local linear model on the training data. For that, you can use function npreg the package np. Choose ll for the method (local linear), and a bandwidth of 0.01; Call this model ll.fit.highflex*  
```{r step 2B, echo=TRUE}
ll.fit.highflex <- npreg(y ~ x, bws = 0.01, method="ll", data=training)
```

#### Step 3: *Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex, on only the training data. See Figure 1.*  
```{r step 3B, echo = TRUE}
training <- training %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = training), y.ll.highflex = predict(object = ll.fit.highflex, newdata = training))

ggplot(training) + geom_point(mapping = aes(x = x, y = y))+geom_line(mapping = aes(x = x, y = y.true))+
  geom_line(mapping = aes(x = x, y=y.ll.lowflex, col="red"))+
  geom_line(mapping = aes(x=x, y=y.ll.highflex, col="blue"))+
  labs(x="x",y="y",caption="Figure 1: Step 3 - Predictions of ll.fit.lowflex and ll.fit.highflex on training data")
```

#### Step 4: *Between the two models, which predictions are more variable? Which predictions have the least bias?*  
```{r step4}
var(training$y.ll.lowflex)
var(training$y.ll.highflex)
var(training$y)

mean(training$y.ll.lowflex)
mean(training$y.ll.highflex)
mean(training$y)
```

The y.ll.lowflex has the smallest variance but its mean is not as close from y as the mean of y.ll.highflex. This means that y.ll.highflex is the one with the smallest bias. The lowflex prediction is the one with the lowest variance but the highflex is the one with the lowest bias. We can see it on the graph: the lowflex predictions form a smooth line while the highflex ones are more disperced. But the highflex predictions are closer from the values of y, this is why it has the lowest bias. 

#### Step 5: *Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex now using the test data. Which predictions are more variable? What happened to the bias of the least biased model?*  
```{r step 5B, echo=TRUE}
test <- test %>% mutate(yt.ll.lowflex = predict(object = ll.fit.lowflex, newdata = test), yt.ll.highflex = predict(object = ll.fit.highflex, newdata = test))

ggplot(test) + geom_point(mapping = aes(x = x, y = y))+geom_line(mapping = aes(x = x, y = y.true))+
  geom_line(mapping = aes(x = x, y=yt.ll.lowflex, col="red"))+
  geom_line(mapping = aes(x=x, y=yt.ll.highflex, col="blue"))+
  labs(x="x",y="y",caption="Figure 1: Step 5 - Predictions of ll.fit.lowflex and ll.fit.highflex on test data")

var(test$yt.ll.lowflex)
var(test$yt.ll.highflex)
var(test$y)

mean(test$yt.ll.lowflex) 
mean(test$yt.ll.highflex)
mean(test$y)
```

The yt.ll.highflex is still the one that fluctuates the most. When we look at the means, the bias of the highflex predictions has increased a lot and now the lowflex predictions are the ones with less bias.   

#### Step 6: *Create a vector of bandwidth going from 0.01 to 0.5 with a step of 0.001*  
```{r step 6B, echo=TRUE}
band <- seq(0.01, 0.5, by = 0.001)
```

#### Step 7: *Estimate a local linear model y ~ x on the training data with each bandwidth.*  
```{r step 7B, echo = TRUE}
llmodel <- lapply(X = band, FUN = function(band) {npreg(y ~ x, data = training, method = "ll", bws = band)})

```

#### Step 8: *Compute for each bandwidth the MSE on the training data.*   
```{r step 8B, echo=TRUE}
MSEtraining <- function(fit.model){
  predi <- predict(object = fit.model, newdata = training)
  training %>% mutate(sqrderr = (y - predi)^2) %>% summarize(mse = mean(sqrderr))
}
MSET <- unlist(lapply(X = llmodel, FUN = MSEtraining))
MSET
```

#### Step 9: *Compute for each bandwidth the MSE on the test data.*  
```{r step 9B, echo=TRUE}
MSEtest <- function(fit.model){
  predi <- predict(object = fit.model, newdata = test)
  test %>% mutate(sqrderr = (y - predi)^2) %>% summarize(mse = mean(sqrderr))
}
MSETEST <- unlist(lapply(X = llmodel, FUN = MSEtest))
MSETEST
```

#### Step 10: *Draw on the same plot how the MSE on training data, and test data, change when the bandwidth increases. Conclude.*  
```{r step 10B, echo=TRUE}
ggplot()+geom_line(mapping = aes(x=band, y=MSET),col="green")+
  geom_line(mapping = aes(x=band, y=MSETEST), col="purple")
```

On the training data, the MSE increases with the band. On the test data, the MSE decreases with the band until the band takes the value 0.13. After this value, it increases like the MSE of the training data.


## TASK 3B: Privacy regulation compliance in France  

We upload the needed packages.  
```{r upload the packages for task 3}
library(dplyr)
```

#### Step 1: *Import the CNIL dataset from the Open Data Portal. (1 point)*  
```{r, step1-3B}
SIREN <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
```

#### Step 2: *Show a (nice) table with the number of organizations that has nominated a CNIL per department. HINT : A department in France is uniquely identified by the first two digits of the postcode. (1 point)*  
We only take the 2 first numbers of the "code postal" of the firms and we transform them in factors. Then, by doing *summary*, we count the numbers of firms by department.  
```{r step2-3B}
dep <- substr(SIREN$CODPOS ,start = 1,stop = 2)
dep <- as.factor(dep)
NiceTable <- data.frame(summary(dep))
NiceTable
```

#### Step 3: *Merge the information from the SIREN dataset into the CNIL data. Explain the method you use. HINT : In the SIREN dataset, there are some rows that refer to the same SIREN number, use the most up to date information about each company. (2 points)*  
We found a way to open the big data:  
```{r import data SIREN, eval=FALSE}
library(ff)
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000, next.rows=1000000, nrows=4000000 colClasses=NA)
CNIL3 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000,next.rows=1000000, skip=4000000, nrows=8000000, colClasses=NA)
CNIL4 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000,next.rows=1000000, skip=8000000, colClasses=NA)
```
But it took too much time so we had to find another way to open and use it.  

We decided to open only the 500 000 first rows of the data in order to do the computations. We also need to clean the SIREN data in order to have the same columns in SIREN and in CNIL.
```{r other way}
system.time(CNIL <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 0, nrows = 500000))

CNIL <- CNIL[!duplicated(CNIL$SIREN),]

SIREN <- SIREN[,-118]
SIREN <- SIREN[,-117]
SIREN <- SIREN[,-116]
SIREN <- SIREN[,-115]
SIREN <- SIREN[,-114]
SIREN <- SIREN[,-113]
SIREN <- SIREN[,-112]
SIREN <- SIREN[,-111]
SIREN <- SIREN[,-110]
SIREN <- SIREN[,-109]
SIREN <- SIREN[,-108]
SIREN <- SIREN[,-107]
SIREN <- SIREN[,-106]
SIREN <- SIREN[,-105]
SIREN <- SIREN[,-104]
SIREN <- SIREN[,-103]
SIREN <- SIREN[,-102]
SIREN <- SIREN[,-101]
```

We select the lines of SIREN which have the same siren as in CNIL, we then delete their "twin" rows in CNIL and replace them by the one of SIREN to update the CNIL data.   
```{r step3B computations}
CNILindex <- SIREN[SIREN$SIREN %in% CNIL$SIREN,]
CNIL <- CNIL[-(CNIL$SIREN %in% SIREN$SIREN),]
CNIL3 <- rbind(CNIL, CNILindex)
```

Here is the code to continue for all the data but it takes a lot of time, this is why we do not run it.  
```{r code of computation nor runned, eval=FALSE}
CNIL2 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 500000, nrows = 1000000)
CNIL2 <- CNIL2[!duplicated(CNIL2$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL2$SIREN,]
CNIL2 <- CNIL2[-(CNIL2$SIREN %in% SIREN$SIREN),]
CNILf2 <- rbind(CNIL2, CNILindex)

CNIL3 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 1500000, nrows = 1000000)
CNIL3 <- CNIL3[!duplicated(CNIL3$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL3$SIREN,]
CNIL3 <- CNIL3[-(CNIL3$SIREN %in% SIREN$SIREN),]
CNILf3 <- rbind(CNIL3, CNILindex)

CNIL4 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 2500000, nrows = 1000000)
CNIL4 <- CNIL4[!duplicated(CNIL4$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL4$SIREN,]
CNIL4 <- CNIL4[-(CNIL4$SIREN %in% SIREN$SIREN),]
CNILf4 <- rbind(CNIL4, CNILindex)

CNIL5 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 3500000, nrows = 1000000)
CNIL5 <- CNIL5[!duplicated(CNIL5$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL5$SIREN,]
CNIL5 <- CNIL5[-(CNIL5$SIREN %in% SIREN$SIREN),]
CNILf5 <- rbind(CNIL5, CNILindex)

CNIL6 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 4500000, nrows = 1000000)
CNIL6 <- CNIL6[!duplicated(CNIL6$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL6$SIREN,]
CNIL6 <- CNIL6[-(CNIL6$SIREN %in% SIREN$SIREN),]
CNILf6 <- rbind(CNIL6, CNILindex)

CNIL7 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 5500000, nrows = 1000000)
CNIL7 <- CNIL7[!duplicated(CNIL7$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL7$SIREN,]
CNIL7 <- CNIL7[-(CNIL7$SIREN %in% SIREN$SIREN),]
CNILf7 <- rbind(CNIL7, CNILindex)

CNIL8 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 6500000, nrows = 1000000)
CNIL8 <- CNIL8[!duplicated(CNIL8$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL8$SIREN,]
CNIL8 <- CNIL8[-(CNIL8$SIREN %in% SIREN$SIREN),]
CNILf8 <- rbind(CNIL8, CNILindex)

CNIL9 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 7500000, nrows = 1000000)
CNIL9 <- CNIL9[!duplicated(CNIL9$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL9$SIREN,]
CNIL9 <- CNIL9[-(CNIL9$SIREN %in% SIREN$SIREN),]
CNILf9 <- rbind(CNIL9, CNILindex)

CNIL10 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 8500000, nrows = 1000000)
CNIL10 <- CNIL10[!duplicated(CNIL10$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL10$SIREN,]
CNIL10 <- CNIL10[-(CNIL10$SIREN %in% SIREN$SIREN),]
CNILf10 <- rbind(CNIL10, CNILindex)

CNIL11 <- read.csv(choose.files(), sep=";", dec=".", header = T, skip = 9500000)
CNIL11 <- CNIL11[!duplicated(CNIL11$SIREN),]
CNILindex <- SIREN[SIREN$SIREN %in% CNIL11$SIREN,]
CNIL11 <- CNIL11[-(CNIL11$SIREN %in% SIREN$SIREN),]
CNILf11 <- rbind(CNIL11, CNILindex)
```

#### Step 4: *Plot the histogram of the size of the companies that nominated a CIL. Comment. (1 points)*  

As we did not managed to open the data completely, we decided to open only the data with the rows that we need for the plot.  
```{r step4B}
system.time(CNIL <- read.csv(choose.files(), sep=";", dec=".", header = T, colClasses = c(NA,rep("NULL",78))))
SIREN <- SIREN[,c("SIREN","LIBTEFEN")]
```

Then we do the plot.  
```{r step4B part 2}
plot(factor(CNIL$LIBTEFEN),las=2, cex.name=0.5)
```

The firms with 0 employees (entrepreneur) are the firms that have at most nominated the CIL.  
