period3 <- period1*period2
fars4 <- cbind(fars, period3)
View(fars4)
fars5 <- fars4[period3==1,]
View(fars5)
childseatmisused <- fars5$childseat*fars5$impchldst
lapshouldmisused <- fars5$lapshould*fars5$impstblt
# We put them into our data
fars6 <- cbind(fars5, childseatmisused,lapshouldmisused)
head(fars6)
# We run the model with our new variables excluding the variables related to the misused of the childseat or of the belt to avoid endogeneity problems:
model4 <- lm(death ~ . -impchldst - impstblt, data = fars6)
summary(model4)
summary(model4)
# We look at the coefficients under the use of the HCCME
model4HCCME <- coeftest(model4,vcovHC(model4))
model4HCCME
childseatmisused2 <- fars4$childseat*fars4$impchldst*fars4$period3
lapshouldmisused2 <- fars4$lapshould*fars4$impstblt*fars4$period3
fars7 <- cbind(fars4, childseatmisused2,lapshouldmisused2)
head(fars7)
model5 <- lm(death ~ . -impchldst - impstblt, data = fars7)
summary(model5)
model5HCCME <- coeftest(model5,vcovHC(model5))
model5HCCME
childseatmisused <- fars4$childseat*fars4$impchldst*fars4$period3
lapshouldmisused <- fars4$lapshould*fars4$impstblt*fars4$period3
# We put them into our data
fars5 <- cbind(fars4, childseatmisused,lapshouldmisused)
head(fars5)
model4 <- lm(death ~ . -impchldst - impstblt, data = fars5)
summary(model4)
# We look at the coefficients under the use of the HCCME
model4HCCME <- coeftest(model4,vcovHC(model4))
model4HCCME
load.libraries <- c('tidyverse')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)
```
```{r housing-step1-sol, echo = TRUE}
train <- read.csv(file.choose(), header = T, dec = ".")
test <- read.csv(file.choose(), header = T, dec = ".")
```
Step 2 : What is the number of observations? What is the target/outcome/dependant variable? How many features can you include?
```{r structure}
dim(train)
```
Step 3 : Is your target variable continuous or categorical? What is its class in R? Is this a regression or a classification problem?
```{r housing-step3-sol, echo= TRUE}
#either
class(train$SalePrice)
#or
train # only if train is in tidyverse, ie a tbl_df
```
Step 4 : What are two numeric features in your data?
Step 5: Summarize the numeric variables in your data set in a nice table.
```{r housing-step5-sol, echo= TRUE}
summary(train)
```
Step 6 : Plot the histogram of the numeric variables that you deem to be interesting. (At least 3.) Show me the plots in your pdf.
```{r housing-step6-sol, echo= TRUE}
hist(train$YearBuilt)
hist(train$LotArea)
hist(train$'1stFlrSF') #TAKE CARE SOLVE IT!!!!!!
par(mfrow = c(1,3))
ggplot(train) + geom_histogram(mapping = aes(x = YearBuilt))
ggplot(train) + geom_histogram(mapping = aes(x = LotArea))
ggplot(train) + geom_histogram(mapping = aes(x = `1stFlrSF`))
```
Step 7 : Are there any missing data? How many observations have missing data? How do you solve this problem?
```{r missing data, echo= TRUE}
head(train) # to check the first rows for missing data by eyeballing
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
# here i summarize the data set using the function sum(is.na()); is.na(.) gives me a column that is equal to TRUE when the row has a missing value (NA) or FALSE when it doesn't, so sum(is.na(.)) gives me the number of missing values for a column, then i gather the data to make it nicer, then i drop all the variables that do not have missing observations
```
Checking the first rows of the data indicates that there are columns which have missing values.
There are some variables with a lot of missing observations, and some other variables that have only some missing observations.
Visualization for the missing data (just for fun, you don't need to make this, but learn)
```{r missing data_plot}
plot_Missing <- function(data_in, title = NULL){
temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
temp_df <- temp_df[,order(colSums(temp_df))]
data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
data_temp$m <- as.vector(as.matrix(temp_df))
data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}
plot_Missing(train[,colSums(is.na(train)) > 0])
```
Ideally, if a variable has a lot of missing observations, you should probably take out that variable unless it's very important. In this case, I remove the variables that have more than 100 missing observations, since they are LotFrontage, Alley, FireplaceQu, PoolQC, Fence, MiscFeature. Except Fence or Alley maybe, there are not very critical for determining the price of a house:
```{r missing data 2, echo= TRUE}
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
train <- train %>% select(- one_of(remove.vars))
```
For the rest of the variables with missing values, I remove the observations with the missing values.
```{r missing data 3, echo= TRUE}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, check if you take all missing values like this
# make sure it's all clean : Yes
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
```
Step 8 : How many duplicate observations are there in the dataset? Remove any duplicates.
Solution :
```{r housing-step8-sol}
# Check for duplicated rows.
cat("The number of duplicated rows are", nrow(train) - nrow(unique(train)))
```
Step 9 : Convert all character variables to factors
```{r housing-step9-sol, echo = TRUE}
#Convert character to factors
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character
train %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factors
```
Step 10 : Fit a linear model including all the variables. Eliminate iteratively the least important variables to get to the most parsimonious yet predictive model. Explain your procedure and interpret the results. **NOTE 1 :** You should have an R2 of at least 70%. **NOTE 2 :** Do not use interaction terms.  You can use powers and transformations (square, logs, etc...) of a feature/explanatory variable, but no interactions.
```{r housing-step10-sol, echo = TRUE}
lm_model_1 <- lm(SalePrice ~ ., data= train)
summary(lm_model_1)
sum_lm_model_1 <- summary(lm_model_1)$coefficients #take only the table of coefficients and t stats and pvalues
class(sum_lm_model_1) #is a matrix
significant.vars <- row.names(sum_lm_model_1[sum_lm_model_1[,4] <= 0.01,]) #sum_lm_model_1[,4] is the p-value of each coefficient, here then i choose the variables that have coefficients significant at the 1% level
# choose any selection of such variables and run a more parcimonious model
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)
```
Step 11 : Use the model that you chose in step 10 to make predictions for the test set (found in test.csv). Export your predictions in a .csv file (like the example in sample_submissions.csv) and submit it.
```{r housing-step11-sol, echo = TRUE}
prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
```
Step 12 : How can you judge the quality of your model and predictions? What simple things can you do to improve it?
library(randomForest)
M1 <- randomForest(train$SalePrice ~ .-Id, data = train)
print(M1)
varImpPlot(M1)
M1$importance
remove.vars <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
test <- test %>% select(- one_of(remove.vars))
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
test <- test %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, check if you take all missing values like this
# make sure it's all clean : Yes
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
levels(test$Utilities) <- levels(train$Utilities)
levels(test$Condition2) <- levels(train$Condition2)
levels(test$HouseStyle) <- levels(train$HouseStyle)
levels(test$RoofMatl) <- levels(train$RoofMatl)
levels(test$Exterior1st) <- levels(train$Exterior1st)
levels(test$Exterior2nd) <- levels(train$Exterior2nd)
levels(test$Electrical) <- levels(train$Electrical)
levels(test$GarageQual) <- levels(train$GarageQual)
levels(test$Heating) <- levels(train$Heating)
prediction2 <- data.frame(Id = test$Id, SalePrice_predict = predict(M1, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
View(prediction2)
is.na(prediction2)
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
View(test)
test <- test %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE, is.na(MSZoning) == FALSE, is.na(Utilities) == FALSE, is.na(BsmtQual) == FALSE, is.na(BsmtCond) == FALSE, is.na(KitchenQual) == FALSE, is.na(Functional) == FALSE, is.na(GarageYrBlt) == FALSE, is.na(GarageFinish) == FALSE, is.na(GarageCars) == FALSE, is.na(GarageArea) == FALSE,is.na(GarageQual) == FALSE, is.na(GarageCond) == FALSE, is.na(SaleType) == FALSE)
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
levels(test$Utilities) <- levels(train$Utilities)
levels(test$Condition2) <- levels(train$Condition2)
levels(test$HouseStyle) <- levels(train$HouseStyle)
levels(test$RoofMatl) <- levels(train$RoofMatl)
levels(test$Exterior1st) <- levels(train$Exterior1st)
levels(test$Exterior2nd) <- levels(train$Exterior2nd)
levels(test$Electrical) <- levels(train$Electrical)
levels(test$GarageQual) <- levels(train$GarageQual)
levels(test$Heating) <- levels(train$Heating)
prediction2 <- data.frame(Id = test$Id, SalePrice_predict = predict(M1, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
is.na(prediction2)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30,5)), nrow=30, ncol=3)
knitr::opts_chunk$set(echo = TRUE)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30,5)), nrow=30, ncol=3)
add2 <-function(x{x+2})
add2(2)
add2 <-function(x){x+2}
add2(2)
add2(4)
add2(2,3)
add2(c(2,3))
v <- seq(1:10)
lapply(X=v, FUN = add2)
sapply(X=v, FUN = add2)
sapply(X=v, FUN = add2)
lapply(X=v, FUN = add2)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30,5)), nrow=30, ncol=3)
m
apply(X = m, MARGIN = 2, FUN = mean)
apply(X = m, MARGIN = c(1,2), FUN = add2)
apply(X = m, MARGIN = c(1,2), FUN = add2) #it will apply the function to each elements
m
apply(X = m, MARGIN = c(1,2), FUN = add2) #it will apply the function to each elements
string1 <- "this is the last lecture"
string2 <- "2"
paste(string1,string2)
paste(string1, string2, sep = ",")
paste(string1, string2, sep = "")
paste0(string1, string2)
library(stringr)
names <- c("rossi", "john", "tim", "arthur")
str_sub(names, start = 3, end = 3)
str_sub(names, start = 3, end = 4)
load.libraries <- c('tidyverse')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)
train <- read.csv(file.choose(), header = T, dec = ".")
test <- read.csv(file.choose(), header = T, dec = ".")
dim(train)
class(train$SalePrice)
#or
train # only if train is in tidyverse, ie a tbl_df
```
Step 4 : What are two numeric features in your data?
Step 5: Summarize the numeric variables in your data set in a nice table.
```{r housing-step5-sol, echo= TRUE}
summary(train)
head(train) # to check the first rows for missing data by eyeballing
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
# here i summarize the data set using the function sum(is.na()); is.na(.) gives me a column that is equal to TRUE when the row has a missing value (NA) or FALSE when it doesn't, so sum(is.na(.)) gives me the number of missing values for a column, then i gather the data to make it nicer, then i drop all the variables that do not have missing observations
plot_Missing <- function(data_in, title = NULL){
temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
temp_df <- temp_df[,order(colSums(temp_df))]
data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
data_temp$m <- as.vector(as.matrix(temp_df))
data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}
plot_Missing(train[,colSums(is.na(train)) > 0])
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
train <- train %>% select(- one_of(remove.vars))
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, check if you take all missing values like this
# make sure it's all clean : Yes
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
cat("The number of duplicated rows are", nrow(train) - nrow(unique(train)))
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character
train %>% mutate_at(.cols = cat_var, .funs = as.factor)
lm_model_1 <- lm(SalePrice ~ ., data= train)
summary(lm_model_1)
sum_lm_model_1 <- summary(lm_model_1)$coefficients #take only the table of coefficients and t stats and pvalues
class(sum_lm_model_1) #is a matrix
significant.vars <- row.names(sum_lm_model_1[sum_lm_model_1[,4] <= 0.01,]) #sum_lm_model_1[,4] is the p-value of each coefficient, here then i choose the variables that have coefficients significant at the 1% level
# choose any selection of such variables and run a more parcimonious model
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)
prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
library(randomForest)
M1 <- randomForest(train$SalePrice ~ .-Id, data = train)
print(M1)
varImpPlot(M1)
M1$importance
library(randomForest)
M1 <- randomForest(train$SalePrice ~ .-Id, data = train)
load.libraries <- c('tidyverse')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)
```
```{r housing-step1-sol, echo = TRUE}
train <- read.csv(file.choose(), header = T, dec = ".")
test <- read.csv(file.choose(), header = T, dec = ".")
```
Step 2 : What is the number of observations? What is the target/outcome/dependant variable? How many features can you include?
```{r structure}
dim(train)
```
Step 3 : Is your target variable continuous or categorical? What is its class in R? Is this a regression or a classification problem?
```{r housing-step3-sol, echo= TRUE}
#either
class(train$SalePrice)
#or
train # only if train is in tidyverse, ie a tbl_df
```
Step 4 : What are two numeric features in your data?
Step 5: Summarize the numeric variables in your data set in a nice table.
```{r housing-step5-sol, echo= TRUE}
summary(train)
```
Step 6 : Plot the histogram of the numeric variables that you deem to be interesting. (At least 3.) Show me the plots in your pdf.
```{r housing-step6-sol, echo= TRUE}
hist(train$YearBuilt)
hist(train$LotArea)
hist(train$'1stFlrSF') #TAKE CARE SOLVE IT!!!!!!
par(mfrow = c(1,3))
ggplot(train) + geom_histogram(mapping = aes(x = YearBuilt))
ggplot(train) + geom_histogram(mapping = aes(x = LotArea))
ggplot(train) + geom_histogram(mapping = aes(x = `1stFlrSF`))
```
Step 7 : Are there any missing data? How many observations have missing data? How do you solve this problem?
```{r missing data, echo= TRUE}
head(train) # to check the first rows for missing data by eyeballing
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
# here i summarize the data set using the function sum(is.na()); is.na(.) gives me a column that is equal to TRUE when the row has a missing value (NA) or FALSE when it doesn't, so sum(is.na(.)) gives me the number of missing values for a column, then i gather the data to make it nicer, then i drop all the variables that do not have missing observations
```
Checking the first rows of the data indicates that there are columns which have missing values.
There are some variables with a lot of missing observations, and some other variables that have only some missing observations.
Visualization for the missing data (just for fun, you don't need to make this, but learn)
```{r missing data_plot}
plot_Missing <- function(data_in, title = NULL){
temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
temp_df <- temp_df[,order(colSums(temp_df))]
data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
data_temp$m <- as.vector(as.matrix(temp_df))
data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}
plot_Missing(train[,colSums(is.na(train)) > 0])
```
Ideally, if a variable has a lot of missing observations, you should probably take out that variable unless it's very important. In this case, I remove the variables that have more than 100 missing observations, since they are LotFrontage, Alley, FireplaceQu, PoolQC, Fence, MiscFeature. Except Fence or Alley maybe, there are not very critical for determining the price of a house:
```{r missing data 2, echo= TRUE}
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
train <- train %>% select(- one_of(remove.vars))
```
For the rest of the variables with missing values, I remove the observations with the missing values.
```{r missing data 3, echo= TRUE}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, check if you take all missing values like this
# make sure it's all clean : Yes
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
```
Step 8 : How many duplicate observations are there in the dataset? Remove any duplicates.
Solution :
```{r housing-step8-sol}
# Check for duplicated rows.
cat("The number of duplicated rows are", nrow(train) - nrow(unique(train)))
```
Step 9 : Convert all character variables to factors
```{r housing-step9-sol, echo = TRUE}
#Convert character to factors
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character
train %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factors
```
Step 10 : Fit a linear model including all the variables. Eliminate iteratively the least important variables to get to the most parsimonious yet predictive model. Explain your procedure and interpret the results. **NOTE 1 :** You should have an R2 of at least 70%. **NOTE 2 :** Do not use interaction terms.  You can use powers and transformations (square, logs, etc...) of a feature/explanatory variable, but no interactions.
```{r housing-step10-sol, echo = TRUE}
lm_model_1 <- lm(SalePrice ~ ., data= train)
summary(lm_model_1)
sum_lm_model_1 <- summary(lm_model_1)$coefficients #take only the table of coefficients and t stats and pvalues
class(sum_lm_model_1) #is a matrix
significant.vars <- row.names(sum_lm_model_1[sum_lm_model_1[,4] <= 0.01,]) #sum_lm_model_1[,4] is the p-value of each coefficient, here then i choose the variables that have coefficients significant at the 1% level
# choose any selection of such variables and run a more parcimonious model
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)
```
Step 11 : Use the model that you chose in step 10 to make predictions for the test set (found in test.csv). Export your predictions in a .csv file (like the example in sample_submissions.csv) and submit it.
```{r housing-step11-sol, echo = TRUE}
prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
```
Step 12 : How can you judge the quality of your model and predictions? What simple things can you do to improve it?
**Let's now download the needed libraries for the Task 1B**
```{r library step, echo=TRUE}
library(randomForest)
```
Step 1:
M1 <- randomForest(train$SalePrice ~ .-Id, data = train)
print(M1)
varImpPlot(M1)
remove.vars <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
test <- test %>% select(- one_of(remove.vars))
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
test <- test %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE, is.na(MSZoning) == FALSE, is.na(Utilities) == FALSE, is.na(BsmtQual) == FALSE, is.na(BsmtCond) == FALSE, is.na(KitchenQual) == FALSE, is.na(Functional) == FALSE, is.na(GarageYrBlt) == FALSE, is.na(GarageFinish) == FALSE, is.na(GarageCars) == FALSE, is.na(GarageArea) == FALSE,is.na(GarageQual) == FALSE, is.na(GarageCond) == FALSE, is.na(SaleType) == FALSE)
# remove rows with NA in some of these variables, check if you take all missing values like this
# make sure it's all clean :
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
# We need to adjust the levels between the training and testing data:
levels(test$Utilities) <- levels(train$Utilities)
levels(test$Condition2) <- levels(train$Condition2)
levels(test$HouseStyle) <- levels(train$HouseStyle)
levels(test$RoofMatl) <- levels(train$RoofMatl)
levels(test$Exterior1st) <- levels(train$Exterior1st)
levels(test$Exterior2nd) <- levels(train$Exterior2nd)
levels(test$Electrical) <- levels(train$Electrical)
levels(test$GarageQual) <- levels(train$GarageQual)
levels(test$Heating) <- levels(train$Heating)
prediction2 <- data.frame(Id = test$Id, SalePrice_predict = predict(M1, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
is.na(prediction2)
prediction2
prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
summary(prediction2)
prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
summary(prediction)
install.packages("pscl")
?pR2
summary(prediction2)
ggplot(test)+geom_line(mapping = aes(x=Id, y=prediction2, col="red"))+
geom_line(mapping = aes(x=Id, y=prediction, col="blue"))
summary(prediction2)
summary(prediction)
ggplot()+geom_line(mapping = aes(x=Id, y=prediction2$SalePrice_predict, col="red"))+
geom_line(mapping = aes(x=Id, y=prediction$SalePrice_predict, col="blue"))
ggplot()+geom_line(mapping = aes(x=prediction2$Id, y=prediction2$SalePrice_predict, col="red"))+
geom_line(mapping = aes(x=prediction$Id, y=prediction$SalePrice_predict, col="blue"))
ggplot()+geom_bar(mapping = aes(x=prediction2$Id, y=prediction2$SalePrice_predict, col="red"))+
geom_bar(mapping = aes(x=prediction$Id, y=prediction$SalePrice_predict, col="blue"))
ggplot()+geom_point(mapping = aes(x=prediction2$Id, y=prediction2$SalePrice_predict, col="red"))+
geom_point(mapping = aes(x=prediction$Id, y=prediction$SalePrice_predict, col="blue"))
SIREN <- read.csv(file.choose(),header = T, dec = ".")
View(SIREN)
install.packages("ff")
SIREN <- read.csv.ffdf(file.choose(),header = T, dec = ".")
library(ff)
SIREN <- read.csv.ffdf(file.choose(), header = T, dec = ".")
install.packages("data.table")
library(data.table)
SIREN <- fread(file.choose(), header = T, dec = ".")
View(SIREN)
library(ff)
CNIL <- read.csv.ffdf(x=NULL, file.choose(), header = TRUE, first.rows = 1000000, next.rows = 1000000, colClasses=NA)
View(prediction)
CNIL <- read.csv(choose.files(), sep=";", dec=".", header = T, colClasses = c(NA,rep("NULL",78)))
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=50000,next.rows=50000, colClasses=NA)
library(ff)
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=50000,next.rows=50000, colClasses=NA)
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000,next.rows=1000000, colClasses=NA)
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000,next.rows=1000000, nrows=4000000 colClasses=NA)
CNIL2 <- read.csv.ffdf(file=file.choose(), header=TRUE, VERBOSE=TRUE, first.rows=1000000,next.rows=1000000, nrows=4000000, colClasses=NA)
SIREN <- read.csv(choose.file(), sep=";", dec=".", header=T)
SIREN <- read.csv(choose.file(), sep=";", dec=".", header=T)
```
SIREN <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
library(dplyr)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
SIREN %>% mutate_at(.cols = cat_var, .funs = as.factor)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
library(dplyr)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
library(tidyr)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
SIREN %>% mutate_at(.cols = cat_var, .funs = as.factor)
SIREN <- SIREN %>% mutate_if(is.factor, funs(factor(replace(., .=="", NA))))
View(SIREN)
SIREN <- ifelse(is.na(SIREN), '.', SIREN)
View(SIREN)
SIREN <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
library(dplyr)
library(tidyr)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
SIREN %>% mutate_at(.cols = cat_var, .funs = as.factor)
SIREN <- SIREN %>% mutate_if(is.factor, funs(factor(replace(., .=="", NA))))
SIREN[is.na(SIREN)] <- "."
View(SIREN)
siren <- read.csv(file="C:/Users/arthur/rprog/challenge_B/cnilgros.csv", header = T, sep = ";", dec = ".", skip=0, nrows = 500000)
siren <- siren[!duplicated(siren$SIREN),]
siren[match(SIREN$SIREN, siren$SIREN),] <- SIREN
SIREN[which(is.na(SIREN)),] <- "."
View(SIREN)
levels(SIREN)<-c(levels(SIREN),".")
SIREN[is.na(SIREN)] <- "."
install.packages("forcats")
fct_explicit_na(SIREN, na_level = ".")
library(forcats)
fct_explicit_na(SIREN, na_level = ".")
fct_explicit_na(as.factor(SIREN), na_level = ".")
SIREN %<>% mutate(cols = fct_explicit_na(col, na_level = "."))
library(dplyr)
library(tidyr)
SIREN %<>% mutate(cols = fct_explicit_na(col, na_level = "."))
library(magrittr)
SIREN %<>% mutate(cols = fct_explicit_na(col, na_level = "."))
library(dplyr)
library(tidyr)
SIREN %<>% mutate(cols = fct_explicit_na(col, na_level = "."))
library(forcats)
SIREN %<>% mutate(cols = fct_explicit_na(col, na_level = "."))
siren <- siren[(SIREN$SIREN %in% siren$SIREN)]
siren <- siren[(SIREN$SIREN %in% siren$SIREN),]
View(siren)
View(SIREN)
cat_var <- SIREN %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist
SIREN <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
CIL <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
sum(is.na(CIL))
CIL<-na.omit(CIL)
dep<-substr(CIL$Code_Postal,start = 1,stop = 2)
CIL$departement<-dep
Nicetable <- as.data.frame(table(departement))
colnames(table)<-c("Department","Number of CNIL")
table
sum(is.na(CIL))
CIL<-na.omit(CIL)
dep<-substr(CIL$Code_Postal,start = 1,stop = 2)
CIL$departement<-dep
Nicetable <- as.data.frame(table(dep))
colnames(table)<-c("Department","Number of CNIL")
dep<-substr(CIL$Code_Postal,start = 1,stop = 2)
dep <- as.factor(dep)
depnice <- data.frame(summary(dep))
depnice
CIL <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
dep<-substr(CIL$Code_Postal,start = 1,stop = 2)
dep <- as.factor(dep)
depnice <- data.frame(summary(dep))
depnice
attach(CIL)
CIL <- read.csv(choose.files(), header = T, sep = ";", dec = ".")
attach(CIL)
setwd("C:/Users/arthur/rprog/challenge_B")
